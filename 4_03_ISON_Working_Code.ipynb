{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ISO_Networks as ISON\n",
    "import datawrangler\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stop_words=ISON.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD RAW DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C, F, T =ISON.load_universe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmu=C.text.tolist()\n",
    "films=F.text.tolist()\n",
    "television=T.text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD CLEANED DATA\n",
    "    Bag of Words - Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# universe_cmu=[ISON.film_strip(i) for i in cmu]\n",
    "universe_film=[ISON.film_strip(i) for i in films]\n",
    "# universe_tv =[ISON.film_strip(i) for i in television]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Bag of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# string_cmu =[' '.join(bow) for bow in universe_cmu]\n",
    "string_film=[' '.join(bow) for bow in universe_film]\n",
    "# string_tv=[' '.join(bow) for bow in universe_tv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_y = np.array(ISON.labels_film(F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-GRAM Models\n",
    "The code below translates a corpus into a bag of words matrix either via tfidf or simple ngram counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_model(corpus):\n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = stop_words)\n",
    "    matrix=tf.fit_transform(corpus)\n",
    "    feature_names = tf.get_feature_names() \n",
    "    return matrix, feature_names\n",
    "\n",
    "\n",
    "def n_gram_counts_model(corpus):\n",
    "    cv= CountVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = stop_words)\n",
    "\n",
    "    matrix=cv.fit_transform(corpus)\n",
    "    feature_names = cv.get_feature_names() \n",
    "    return matrix, feature_names\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 5s, sys: 54.1 s, total: 5min 59s\n",
      "Wall time: 6min 14s\n"
     ]
    }
   ],
   "source": [
    "%time tfM, ft = tf_idf_model(string_film)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 34s, sys: 1min 4s, total: 5min 39s\n",
      "Wall time: 6min 8s\n"
     ]
    }
   ],
   "source": [
    "%time tfC, ftC = n_gram_counts_model(string_film)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2827, 14130866)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfC.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model\n",
    "The vectors obtained by the simple bag of words model above are then fed into a simple logistic regression model below. The ISON_Film_experiment function takes the sparse vector matrix of the film corpus, the labels, and a desired test size and trains a logistic regression classifier on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def train_test(data, labels, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                    stratify=labels, \n",
    "                                                    test_size=test_size)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def ISON_Film_experiment(universe_representation, labels, test_size, model, dictionary):\n",
    "    \n",
    "    \n",
    "    #STEP 1: SPLIT THE DATA\n",
    "    X_train, X_test, y_train, y_test=train_test(universe_representation,labels,test_size)\n",
    "    print('START')\n",
    "    \n",
    "    #STEP 2: \n",
    "    RLR = RandomizedLogisticRegression()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    label_encoded=le.fit_transform(labels)\n",
    "    randomizedLRM=RLR.fit(universe_representation, label_encoded)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    feature_scores=randomizedLRM.all_scores_\n",
    "    index = np.where(feature_scores!= 0)[0]\n",
    "    \n",
    "    model_ML = Pipeline([\n",
    "    ('clf',OneVsRestClassifier(LogisticRegression(solver='sag')))\n",
    "                                                                    ])\n",
    "    \n",
    "    print('Fitting model')\n",
    "    clf=model_ML.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('\\n')\n",
    "    print(classification_report( y_test, y_pred))\n",
    "    print((y_pred == y_test).mean())\n",
    "    \n",
    "    print('%s features were of importance, the following are the weights:' %len(index))\n",
    "    print(feature_scores[index])\n",
    "    print('#'*33, '\\n')\n",
    "    \n",
    "    topics = datawrangler.topic_items(model, 20)\n",
    "    for i in index:\n",
    "        print(feature_scores[i],i,  topics[i])\n",
    "\n",
    "    \n",
    "    print('#'*33, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ViVeri/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class RandomizedLogisticRegression is deprecated; The class RandomizedLogisticRegression is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with CountVectorizer()\n",
    "%time ISON_Film_experiment(tfM,data_y, .20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Fitting model\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Action       0.02      0.02      0.02        58\n",
      "  Adventure       0.00      0.00      0.00        33\n",
      "     Comedy       0.00      0.00      0.00        70\n",
      "      Crime       0.00      0.00      0.00        40\n",
      "      Drama       0.20      0.57      0.30       116\n",
      "    Fantasy       0.00      0.00      0.00        23\n",
      "     Horror       0.00      0.00      0.00        30\n",
      "    Mystery       0.00      0.00      0.00        21\n",
      "      Other       0.22      0.06      0.10        31\n",
      "    Romance       0.00      0.00      0.00        38\n",
      "     Sci-Fi       0.00      0.00      0.00        31\n",
      "   Thriller       0.00      0.00      0.00        75\n",
      "\n",
      "avg / total       0.05      0.12      0.07       566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ViVeri/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.12190812720848057\n",
      "################################# \n",
      "\n",
      "CPU times: user 11min 40s, sys: 23 s, total: 12min 3s\n",
      "Wall time: 12min 29s\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with Tfidf_vectorizer()\n",
    "%time ISON_Film_experiment(tfM,data_y, .20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Models\n",
    "The code below implements the second modeling technique which utilizes a topic model generate from one corpus to model topics in a second corpus. In this case the topic model generate was trained on the TV corpus, and then this model was used to create a bag of topics model in which each document in the FILM corpus is reprsented by a vector of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a dense M x T matrix to train classifier on\n",
    "def document_theme_vector(data, model, dictionary):\n",
    "    theme_matrix = []\n",
    "    weight =lambda x: np.array([i[1] for i in x])\n",
    "    \n",
    "    for i in data:\n",
    "        theme_array=model.get_document_topics(dictionary.doc2bow(i), minimum_probability=0)\n",
    "        vector=weight(theme_array)\n",
    "        theme_matrix.append(vector)\n",
    "    \n",
    "\n",
    "    theme_matrix = np.array(theme_matrix)\n",
    "    return theme_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and use to vectorize training corpus\n",
    "model, dictionary = datawrangler.load_model(\"tvTOP50_50.model\")\n",
    "universe_film=ISON.depickler('universe_film.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.9 s, sys: 3.13 s, total: 57 s\n",
      "Wall time: 54.8 s\n"
     ]
    }
   ],
   "source": [
    "%time M=document_theme_vector(universe_film,model, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = ISON.labels_film(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ViVeri/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class RandomizedLogisticRegression is deprecated; The class RandomizedLogisticRegression is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Action       0.33      0.12      0.18        58\n",
      "  Adventure       0.00      0.00      0.00        33\n",
      "     Comedy       0.52      0.21      0.30        70\n",
      "      Crime       0.00      0.00      0.00        40\n",
      "      Drama       0.23      0.84      0.36       116\n",
      "    Fantasy       0.00      0.00      0.00        23\n",
      "     Horror       0.00      0.00      0.00        30\n",
      "      Other       0.00      0.00      0.00        52\n",
      "    Romance       0.00      0.00      0.00        38\n",
      "     Sci-Fi       0.40      0.06      0.11        31\n",
      "   Thriller       0.17      0.19      0.18        75\n",
      "\n",
      "avg / total       0.19      0.24      0.16       566\n",
      "\n",
      "0.23851590106007067\n",
      "28 features were of importance, the following are the weights:\n",
      "[[0.565]\n",
      " [0.105]\n",
      " [0.47 ]\n",
      " [0.105]\n",
      " [0.61 ]\n",
      " [0.175]\n",
      " [0.8  ]\n",
      " [0.5  ]\n",
      " [0.18 ]\n",
      " [0.25 ]\n",
      " [0.46 ]\n",
      " [0.005]\n",
      " [0.01 ]\n",
      " [0.365]\n",
      " [0.685]\n",
      " [0.32 ]\n",
      " [0.68 ]\n",
      " [0.95 ]\n",
      " [0.125]\n",
      " [0.005]\n",
      " [0.535]\n",
      " [0.005]\n",
      " [0.16 ]\n",
      " [0.96 ]\n",
      " [0.44 ]\n",
      " [0.12 ]\n",
      " [0.12 ]\n",
      " [0.64 ]]\n",
      "################################# \n",
      "\n",
      "[0.565] 0 ['agent', 'work', 'fbi', 'find', 'finch', 'phone', 'security', 'time', 'machine', 'working', 'number', 'year', 'code', 'call', 'file', 'office', 'government', 'cia', 'riker', 'system']\n",
      "[0.105] 1 ['alien', 'supergirl', 'planet', 'earth', 'el', 'city', 'danvers', 'm', 'mon', 'onn', 'human', 'superman', 'national', 'world', 'cousin', 'martian', 'um', 'catco', 'sister', 'ufo']\n",
      "[0.47] 3 ['sir', 'captain', 'ship', 'data', 'enterprise', 'time', 'picard', 'commander', 'system', 'computer', 'power', 'shield', 'aye', 'bridge', 'log', 'warp', 'ready', 'beam', 'sensor', 'cmdr']\n",
      "[0.105] 5 ['house', 'patient', 'didn', 'gonna', 'doctor', 'test', 'blood', 'doesn', 'wrong', 'time', 'heart', 'year', 'yeah', 'give', 'mean', 'pain', 'work', 'people', 'hospital', 'problem']\n",
      "[0.61] 6 ['sigh', 'chuckle', 'gasp', 'grunt', 'groan', 'laugh', 'grunting', 'laughing', 'door', 'hmm', 'screaming', 'hey', 'music', 'scream', 'continues', 'playing', 'groaning', 'ringing', 'panting', 'tire']\n",
      "[0.175] 8 ['christmas', 'holiday', 'ho', 'tree', 'present', 'year', 'gift', 'family', 'coon', 'god', 'snow', 'toy', 'chrismukkah', 'hankey', 'spirit', 'jewish', 'miracle', 'bell', 'dun', 'time']\n",
      "[0.8] 10 ['hey', 'show', 'movie', 'guy', 'yeah', 'god', 'whoa', 'gonna', 'people', 'time', 'ooh', 'ah', 'dad', 'krusty', 'wow', 'huh', 'cool', 'kid', 'aw', 'aah']\n",
      "[0.5] 11 ['shit', 'fuck', 'fucking', 'gonna', 'ain', 'yeah', 'yo', 'as', 'money', 'motherfucker', 'gotta', 'boy', 'bullshit', 'bitch', 'fucked', 'give', 'asshole', 'call', 'work', 'drug']\n",
      "[0.18] 15 ['team', 'game', 'vote', 'coach', 'football', 'school', 'election', 'ball', 'sport', 'baseball', 'luthor', 'prom', 'field', 'kryptonite', 'stadium', 'campaign', 'voting', 'fight', 'high', 'league']\n",
      "[0.25] 16 ['member', 'vampire', 'funny', 'camp', 'world', 'holodeck', 'space', 'water', 'ah', 'troll', 'time', 'scout', 'computer', 'bleep', 'ralphie', 'phaser', 'facebook', 'shark', 'online', 'je']\n",
      "[0.46] 17 ['lord', 'father', 'men', 'stark', 'brother', 'khan', 'north', 'army', 'war', 'wall', 'mother', 'child', 'boy', 'time', 'girl', 'dragon', 'sword', 'fight', 'horse', 'castle']\n",
      "[0.005] 19 ['panda', 'treasure', 'passport', 'war', 'sexual', 'harrassment', 'manbearpig', 'schiff', 'bilac', 'stonecutter', 'cave', 'founding', 'protest', 'hathaway', 'rat', 'stans', 'gore', 'garper', 'museum', 'clue']\n",
      "[0.01] 21 ['daly', 'hume', 'antichrist', 'moriarty', 'converter', 'packer', 'warcraft', 'kamin', 'nemo', 'shazia', 'valdack', 'crystalline', 'character', 'inorganic', 'kif', 'kabir', 'dudani', 'jayden', 'nautilus', 'triolic']\n",
      "[0.365] 23 ['president', 'sir', 'yeah', 'call', 'gonna', 'bauer', 'minute', 'time', 'find', 'people', 'phone', 'country', 'give', 'agent', 'team', 'move', 'ctu', 'talk', 'understand', 'hour']\n",
      "[0.685] 26 ['yeah', 'people', 'time', 'gonna', 'didn', 'find', 'call', 'work', 'hey', 'place', 'cop', 'police', 'thing', 'lot', 'friend', 'kind', 'bad', 'hand', 'real', 'dead']\n",
      "[0.32] 27 ['gonna', 'baby', 'god', 'time', 'woman', 'sex', 'wanna', 'feel', 'yeah', 'girl', 'hey', 'people', 'kid', 'gotta', 'fine', 'big', 'work', 'bad', 'didn', 'thing']\n",
      "[0.68] 28 ['dad', 'boy', 'time', 'kid', 'give', 'mom', 'simpson', 'hey', 'year', 'school', 'hmm', 'won', 'big', 'sir', 'money', 'child', 'house', 'father', 'god', 'people']\n",
      "[0.95] 30 ['planet', 'time', 'year', 'earth', 'worf', 'ship', 'colonel', 'rush', 'light', 'world', 'space', 'universe', 'capt', 'science', 'energy', 'alien', 'water', 'star', 'system', 'ensign']\n",
      "[0.125] 31 ['bee', 'flute', 'guinea', 'band', 'peruvian', 'hive', 'peru', 'pandemic', 'mt', 'splashmore', 'pig', 'aguatecture', 'liddell', 'finland', 'kiku', 'gracias', 'grasso', 'pan', 'gak', 'gronk']\n",
      "[0.005] 33 ['bassam', 'american', 'country', 'father', 'thing', 'um', 'people', 'talk', 'wife', 'brother', 'war', 'soviet', 'ihab', 'state', 'russian', 'family', 'mm', 'centre', 'general', 'hmm']\n",
      "[0.535] 35 ['fuckin', 'yeah', 'dang', 'huh', 'hey', 'car', 'money', 'ya', 'uncle', 'sir', 'propane', 'call', 'gotta', 'ol', 'business', 'spanish', 'soprano', 'bos', 'house', 'truck']\n",
      "[0.005] 36 ['gotham', 'penguin', 'bullock', 'war', 'falcone', 'nazir', 'abu', 'hand', 'caliphate', 'family', 'master', 'abuddin', 'gcpd', 'nygma', 'barnes', 'child', 'galavan', 'strange', 'kill', 'people']\n",
      "[0.16] 37 ['didn', 'gonna', 'time', 'client', 'doesn', 'money', 'give', 'hell', 'work', 'deal', 'care', 'lawyer', 'office', 'wanted', 'talking', 'shit', 'year', 'isn', 'talk', 'call']\n",
      "[0.96] 38 ['yeah', 'hey', 'gonna', 'time', 'didn', 'god', 'guy', 'talk', 'friend', 'mom', 'work', 'dad', 'call', 'girl', 'feel', 'fine', 'nice', 'year', 'kind', 'lot']\n",
      "[0.44] 40 ['time', 'gonna', 'kill', 'people', 'find', 'father', 'didn', 'yeah', 'dead', 'happened', 'hell', 'mother', 'killed', 'understand', 'give', 'stay', 'won', 'die', 'talk', 'coming']\n",
      "[0.12] 47 ['mulder', 'agent', 'scully', 'yeah', 'body', 'find', 'sir', 'doggett', 'men', 'happened', 'dead', 'file', 'car', 'victim', 'year', 'scofield', 'evidence', 'death', 'didn', 'fbi']\n",
      "[0.12] 48 ['island', 'boat', 'sayid', 'plane', 'dude', 'jungle', 'ain', 'beach', 'lito', 'widmore', 'shephard', 'camp', 'fish', 'push', 'whisper', 'thv', 'monorail', 'rajan', 'wanna', 'chinpokomon']\n",
      "[0.64] 49 ['music', 'yeah', 'time', 'gonna', 'flash', 'dramatic', 'power', 'cisco', 'hey', 'speed', 'earth', 'world', 'guy', 'future', 'city', 'human', 'zoom', 'um', 'lab', 'didn']\n",
      "################################# \n",
      "\n",
      "CPU times: user 6.3 s, sys: 158 ms, total: 6.46 s\n",
      "Wall time: 6.51 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ViVeri/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%time ISON_Film_experiment(M, data_y, .20, model, dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
